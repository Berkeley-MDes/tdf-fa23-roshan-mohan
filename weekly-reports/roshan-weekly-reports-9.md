Last Thursday, we held a project review session where we had the opportunity to explore and gain inspiration from the work submitted by our fellow students.

During a session led by Peter Binggeser titled "Designing Intelligence," we delved into the subject of tailoring Language Models (LLMs). Here's a concise summary of the key steps we covered:

Pretraining: This initial phase entails feeding a substantial volume of raw text (tokens) through a substantial neural network.

Fine Tuning: In this step, the model is refined through structured supervised prompt and response example data. An illustration of this method involves using models such as Hugging Face/OpenAI.

RLHF (Reinforcement Learning from Human Feedback): In this stage, the model generates various options and selects the most promising one to enhance its foundational structure, rendering it more adaptable.

The conversation offered valuable insights into the process of customizing language models, contributing significantly to our understanding of the topic, and it was a highly instructive experience.
